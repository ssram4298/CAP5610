# -*- coding: utf-8 -*-
"""ML comp1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uBS5wuivsNqq3HKjGQzo4VGHf4Hpl8lF
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

# Install tsflex and seglearn
!pip install tsflex --no-index --find-links=file:///kaggle/input/competetion-ds/dataset
!pip install seglearn --no-index --find-links=file:////kaggle/input/competetion-ds/dataset

import numpy as np
import pandas as pd
from sklearn import *
import glob
import os

data_path = '/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/'
train_files = glob.glob(os.path.join(data_path, 'train', '**', '**'))
test_files = glob.glob(os.path.join(data_path, 'test', '**', '**'))
subjects = pd.read_csv(os.path.join(data_path, 'subjects.csv'))
tasks = pd.read_csv(os.path.join(data_path, 'tasks.csv'))
sample_submission = pd.read_csv(os.path.join(data_path, 'sample_submission.csv'))

tdcsfog_metadata = pd.read_csv(os.path.join(data_path, 'tdcsfog_metadata.csv'))
defog_metadata = pd.read_csv(os.path.join(data_path, 'defog_metadata.csv'))
tdcsfog_metadata['Module'] = 'tdcsfog'
defog_metadata['Module'] = 'defog'
metadata = pd.concat([tdcsfog_metadata, defog_metadata])

import pandas as pd
from sklearn.cluster import KMeans

tasks = pd.read_csv(p+'tasks.csv')
tasks['Duration'] = tasks['End'] - tasks['Begin']
tasks = pd.pivot_table(tasks, values=['Duration'], index=['Id'], columns=['Task'], aggfunc='sum', fill_value=0)
tasks.columns = [f'task_{c[-1]}' for c in tasks.columns]
tasks = tasks.reset_index()
tasks['t_kmeans'] = KMeans(n_clusters=10, random_state=3).fit_predict(tasks.iloc[:, 1:])

subjects = pd.read_csv(p+'subjects.csv')
subjects = subjects.fillna(0).groupby('Subject').median()
subjects = subjects.rename(columns={'Visit':'s_Visit','Age':'s_Age','YearsSinceDx':'s_YearsSinceDx','UPDRSIII_On':'s_UPDRSIII_On','UPDRSIII_Off':'s_UPDRSIII_Off','NFOGQ':'s_NFOGQ'})
subjects['s_kmeans'] = KMeans(n_clusters=10, random_state=3).fit_predict(subjects.iloc[:, 1:])

print(tasks)
print(subjects)

complex_featlist=['Visit','Test','Medication','s_Visit','s_Age','s_YearsSinceDx','s_UPDRSIII_On','s_UPDRSIII_Off','s_NFOGQ','s_kmeans']
metadata_complex=metadata.merge(subjects,how='left',on='Subject').copy()
metadata_complex['Medication']=metadata_complex['Medication'].factorize()[0]

display(metadata_complex)

import seglearn.feature_functions as sf
from tsflex.features import FeatureCollection, MultipleFeatureDescriptors
from tsflex.features.integrations import seglearn_feature_dict_wrapper

basic_feats = MultipleFeatureDescriptors(
    functions=seglearn_feature_dict_wrapper(sf.base_features()),
    series_names=['AccV', 'AccML', 'AccAP'],
    windows=[5000],
    strides=[5000],
)

emg_feats = sf.emg_features()
emg_feats.pop('simple square integral') # is same as abs_energy (which is in base_features)

emg_feats = MultipleFeatureDescriptors(
    functions=seglearn_feature_dict_wrapper(emg_feats),
    series_names=['AccV', 'AccML', 'AccAP'],
    windows=[5000],
    strides=[5000],
)

fc = FeatureCollection([basic_feats, emg_feats])

import pathlib

def reader(f):
    try:
        df = pd.read_csv(f, index_col="Time", usecols=['Time', 'AccV', 'AccML', 'AccAP', 'StartHesitation', 'Turn' , 'Walking'])
        df['Id'] = os.path.basename(f).split('.')[0]
        df['Module'] = os.path.dirname(f)
        df = pd.merge(df, tasks[['Id','t_kmeans']], how='left', on='Id').fillna(-1)
        df = pd.merge(df, metadata_complex[['Id','Subject']+['Visit','Test','Medication','s_kmeans']], how='left', on='Id').fillna(-1)
        df_feats = fc.calculate(df, return_df=True, include_final_window=True, approve_sparsity=True, window_idx="begin").astype(np.float32)
        df = df.merge(df_feats, how="left", left_index=True, right_index=True)
        df.fillna(method="ffill", inplace=True)
        return df
    except Exception as e:
        print(f"An error occurred while reading file {f}: {e}")

train = pd.concat([reader(f) for f in tqdm(train) if reader(f) is not None]).fillna(0)
print(train.shape)

cols = [c for c in train.columns if c not in ['Id','Subject','Module', 'Time', 'StartHesitation', 'Turn' , 'Walking', 'Valid', 'Task','Event']]
pcols = ['StartHesitation', 'Turn' , 'Walking']
scols = ['Id', 'StartHesitation', 'Turn' , 'Walking']

train=train.reset_index(drop=True)

import xgboost as xgb
import lightgbm as lgb
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_squared_error, r2_score

import numpy as np
import xgboost as xgb
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from scipy.stats import uniform, randint
from sklearn.metrics import average_precision_score, make_scorer

best_params_ = {'estimator__colsample_bytree': 0.50, 
 'estimator__learning_rate': 0.24, 
 'estimator__max_depth': 8, 
 'estimator__min_child_weight': 3.14, 
 'estimator__n_estimators':300, 
 'estimator__subsample': 1}

best_params_ = {kk: v for k, v in best_params_.items() for kk in k.split('__')}; del best_params_['estimator']

from sklearn.base import clone
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import average_precision_score


def custom_average_precision(y_true, y_pred):
    score = average_precision_score(y_true, y_pred)
    return 'average_precision', score, True


class LGBMMultiOutputRegressor(MultiOutputRegressor):
    """
    A multi-output regressor that fits a separate LGBMRegressor for each output.
    """
    def fit(self, X, y, eval_set=None, **fit_params):
        self.estimators_ = [clone(self.estimator) for _ in range(y.shape[1])]
        
        for i, estimator in enumerate(self.estimators_):
            if eval_set:
                fit_params['eval_set'] = [(eval_set[0], eval_set[1][:, i])]
            super().fit(X, y[:, i], **fit_params)
        
        return self

from sklearn.model_selection import GroupKFold

N_FOLDS = 5
kfold = GroupKFold(N_FOLDS)
group_var = train.Subject
groups = kfold.split(train, groups=group_var)
cvs = []
np.random.seed(42)

for fold, (tr_idx, te_idx) in enumerate(tqdm(groups, total=N_FOLDS, desc="Folds")):
    tr_idx = pd.Series(tr_idx).sample(n=2000000, random_state=42).values
    train_sampled = train.loc[tr_idx].sample(frac=1, random_state=42)  # Shuffle the data
    x_tr, y_tr = train_sampled[cols].to_numpy(), train_sampled[pcols].to_numpy()
    x_te, y_te = train.loc[te_idx, cols].to_numpy(), train.loc[te_idx, pcols].to_numpy()

    base_regressor = lgb.LGBMRegressor(**best_params_)
    multioutput_regressor = LGBMMultiOutputRegressor(base_regressor)

    multioutput_regressor.fit(
        x_tr, y_tr,
        eval_set=(x_te, y_te),
        eval_metric=custom_average_precision,
        early_stopping_rounds=25
    )

    cv = metrics.average_precision_score(y_te, multioutput_regressor.predict(x_te).clip(0.0, 1.0))
    cvs.append(cv)

print(cvs)
print("Average CV score: {:.4f}".format(np.mean(cvs)))

from pathlib import Path

sub['t'] = 0
submission = []
for f in test:
    df = pd.read_csv(f)
    df.set_index('Time', drop=True, inplace=True)
    df['Id'] = Path(f).stem
    
    df = pd.merge(df, tasks[['Id','t_kmeans']], how='left', on='Id').fillna({'t_kmeans': -1})
    df = pd.merge(df, metadata_complex[['Id','Subject']+['Visit','Test','Medication','s_kmeans']], how='left', on='Id').fillna({'s_kmeans': -1})
    
    df_feats = fc.calculate(df, return_df=True, include_final_window=True, approve_sparsity=True, window_idx="begin")
    df = df.merge(df_feats, how="left", left_index=True, right_index=True)
    df.fillna(method="ffill", inplace=True)
    
    res_vals = [np.expand_dims(np.round(regs[i_fold].predict(df[cols]).clip(0.0,1.0),3), axis=2) for i_fold in range(N_FOLDS)]
    res_vals = np.mean(np.concatenate(res_vals, axis=2), axis=2)
    res = pd.DataFrame(res_vals, columns=pcols)
    
    df = pd.concat([df, res], axis=1, inplace=False)
    df['Id'] = df['Id'].astype(str) + '_' + df.index.astype(str
    submission.append(df[scols])
submission = pd.concat(submission)
submission = pd.merge(sub[['Id']], submission, how='left', on='Id').fillna(0.0)
submission[scols].to_csv('submission.csv', index=False)

submission